\title{Taxi Trajectory Prediction}
\author{
        Karl Krauth (z3416790)\\
        David McKinnon (z3421068)
}
\date{\today}

\documentclass[12pt]{article}

\begin{document}
\maketitle

\section{Introduction}
Taxis nowadays use electronic dispatch systems for scheduling pick-ups, but they do not usually enter their drop-off locations. Therefore, when a call comes for a taxi, it is difficult for dispatchers to know which taxi to contact. 

To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Since there is often a taxi whose current ride will end near a requested pick up location from a new passenger, it would be useful to know approximately where each taxi is likely to end so that the system can identify the best taxi to assign to each new pickup request. This lowers the waiting time for new passengers and allows the taxi system to operate more efficiently.

This project occurs in the context of a Kaggle competition hosted by ECML/PKDD. The goal of this competition is to predict the destination of taxis travelling in Porto, Portugal given specific data about the current trip. To aid with this a dataset of around 1.7 million complete trips is provided.

\section{Features and Observations of the Data Set}
The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately $25$\%), the taxi has been called through the taxi call-center, and the passenger's telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id.

Initially, naive Bayesian conditions were graphed to find any potential connections in the data - frequency of taxi rides, given the time of day, or type of day, or taxi request type, or the average heading of the taxi from start to finish given the time/type of day/taxi request. These were plotted in a series of graphs shown below in Appendix 1. 
It was hoped that these could provide some hint as to the final destination - for example, taxis are most frequently called between the hours of X and Y. A 3D histogram was then created to check for a connection between time of day and average heading. This can be seen in Figure b) of Appendix 1. 

In all of these graphs no significant connections were found, except that the overwhelming majority of taxi trips took place not on or before public holidays - but this is obvious. 
The baseline approach was simply to classify all taxi rides as ending in downtown Porto - as the majority went here anyway, this proved effective. 
The approach in this project improved upon this and ended up using none of the connections hypothesised above. 

\section{Methods}
\subsection{Naive Bayes}


\subsection{Regression Trees and Boosting}
The second method that was attempted was the use of regression trees, both used by itself and augmented by AdaBoost. Before being able to train on the dataset we first went through a pre-processing step. The following features were modified as follows:
\begin{itemize}
  \item \textbf{ORIGIN\_CALL:} If set to a NULL value instead set it to 0.
  \item \textbf{ORIGIN\_STAND:} If set to a NULL value instead set it to 0.
  \item \textbf{CALL\_TYPE:} Remove this column since ORIGIN\_CALL and ORIGIN\_STAND give us the information we need about the CALL\_TYPE.
  \item \textbf{DAYTYPE:} Remove the DAYTYPE since they're all set to the same value.
  \item \textbf{MISSING\_DATA:} Remove any rows with missing data (only 10 datapoints have missing data)
  \item \textbf{POLYLINE:} Keep only the starting point, a randomly selected point in the middle and the end point (as the target value).
\end{itemize}
The idea behind the modification of POLYLINE is that we would like to transform a variable length vector into a fixed length vector, we also would like to modify polyline to only be a partial path since a complete path would not help with prediction.

After having modified the dataset we then train a regression tree on the generated training set. A maximum depth of 20 leads to good performance. We also train a regression tree used in conjunction with AdaBoost, however since 

\subsection{Frechet Distance and KNN}
Another method used was a naive Frechet classifier, with the estimated endpoint to be the end of the path which when truncated had the smallest Frechet distance to the partial path. This was tested using a training set of 200 000, and had an average distance of about 7km. 
After some research into k Nearest Neighbours, another approach, the final approach, was considered. 
The method was to read the paths of all the trips given into a ball tree [1], using the distance metric of the Haversine distance [2] between the starting points of the paths. 
For each partial path that needs its destination estimated, the k nearest neighbours to the path are found from the ball tree, using the distance metric with k = 10 000. From these 10 000 paths, each path is truncated to constitute the same distance travelled as the partial path given. Then the partial path is compared to every truncated path using the Frechet distance [3]. The end coordinates of the path with the smallest Frechet distance from the partial path is chosen to be the estimated endpoint. 
With k = 10 000, preliminary results from running on a validation set taken from the given data, of size 300, indicate that the mean error is 2km, which is performing better than all other learners on the Kaggle leaderboards. However, this performed poorly on the actual test set that Kaggle used. This idea is promising, but appears to have overfit the test data. While this does perfomr worse htan the baseline on the evaluation data, getting a score of ~3.8 ish, this could still be practically used, as it would perform better classification on trips that did not end up in downtown Porto. 


\section{Results}
Running cross validation on the given data set with trees of size 5, 8 and 20 yields average distances of 2.28km, 2.09km and 1.72km respectively. Clearly the regression tree performs better with a greater depth, and since cross validation is used, this is unlikely to be overfitting. 

\section{Further Work}

\section{Conclusion}

\section{Appendix}

\end{document}
