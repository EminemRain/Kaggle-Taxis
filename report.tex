\title{Taxi Trajectory Prediction}
\author{
        Karl Krauth (z3416790)\\
        David McKinnon (z3421068)
}
\date{\today}

\documentclass[12pt]{article}

\begin{document}
\maketitle

\section{Introduction}
Taxis nowadays use electronic dispatch systems for scheduling pick-ups, but they do not usually enter their drop-off locations. Therefore, when a call comes for a taxi, it is difficult for dispatchers to know which taxi to contact. 

To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Since there is often a taxi whose current ride will end near a requested pick up location from a new passenger, it would be useful to know approximately where each taxi is likely to end so that the system can identify the best taxi to assign to each new pickup request. This lowers the waiting time for new passengers and allows the taxi system to operate more efficiently.

This project occurs in the context of a Kaggle competition hosted by ECML/PKDD. The goal of this competition is to predict the destination of taxis travelling in Porto, Portugal given specific data about the current trip. To aid with this a dataset of around 1.7 million complete trips is provided.

\section{Features and Observations of the Data Set}
The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately $25$\%), the taxi has been called through the taxi call-center, and the passenger's telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id.

Initially, naive Bayesian conditions were graphed to find any potential connections in the data - frequency of taxi rides, given the time of day, or type of day, or taxi request type, or the average heading of the taxi from start to finish given the time/type of day/taxi request. These were plotted in a series of graphs shown below in Appendix 1. 
It was hoped that these could provide some hint as to the final destination - for example, taxis are most frequently called between the hours of X and Y. A 3D histogram was then created to check for a connection between time of day and average heading. This can be seen in Figure b) of Appendix 1. 

In all of these graphs no significant connections were found, except that the overwhelming majority of taxi trips took place not on or before public holidays - but this is obvious. 
The baseline approach was simply to classify all taxi rides as ending in downtown Porto - as the majority went here anyway, this proved effective. 
The approach in this project improved upon this and ended up using none of the connections hypothesised above. 

\section{Methods}
\subsection{Naive Bayes}


\subsection{Regression Trees and Boosting}
The second method that was attempted was the use of regression trees, both used by itself and augmented by AdaBoost. Before being able to train on the dataset we first went through a pre-processing step. The following features were modified as follows:
\begin{itemize}
  \item \textbf{ORIGIN\_CALL:} If set to a NULL value instead set it to 0.
  \item \textbf{ORIGIN\_STAND:} If set to a NULL value instead set it to 0.
  \item \textbf{CALL\_TYPE:} Remove this column since ORIGIN\_CALL and ORIGIN\_STAND give us the information we need about the CALL\_TYPE.
  \item \textbf{DAYTYPE:} Remove the DAYTYPE since they're all set to the same value.
  \item \textbf{MISSING\_DATA:} Remove any rows with missing data (only 10 datapoints have missing data)
  \item \textbf{POLYLINE:} Keep only the starting point, a randomly selected point in the middle and the end point (as the target value).
\end{itemize}
The idea behind the modification of POLYLINE is that we would like to transform a variable length vector into a fixed length vector, we also would like to modify polyline to only be a partial path since a complete path would not help with prediction.

After having modified the dataset we then train a regression tree on the generated training set. A maximum depth of 20 leads to good performance. We also train a regression tree used in conjunction with AdaBoost, with 5 instances and a max depth of 20 for better performance.

\subsection{Frechet Distance and KNN}
\subsubsection{Frechet Distance}
The final and best performing method that was evaluated only made use of the paths. It used a similarity measure between two oriented curves called the frechet distance. The frechet between two curves $A$ and $B$ is defined as follow:
\[\inf_{\alpha,\beta} \max_{t \in [0,1]} \left( d(A(\alpha(t)), B(\beta(t))) \right).\]
Where $d$ in our case is the haversine distance between two GPS coordinates. Intuitively the frechet distance can be thought as being the minimum length of a leash required to connect a dog and its owner constrained on the two seperate paths, as they walk along with no backtracking. In our case we are dealing with polygonal line segments and thus only need to consider the curves at each vertex, an approach using dynamic programming is given in the implementation.
\subsubsection{Truncating paths.}
We could simply compare the current test data's path with all training paths and pick the destination of the training path with the shortest frechet distance as our prediction. However this does not work well in practice since this favours predictions that assume that the test path is nearly complete (which it is not in the majority of situations). To remedy this we instead chose to truncate training paths that are longer than the test path before comparing them. We define the length of a polygonal path as the haversine distance between every pair of adjacent coordinates in the path.
\subsubsection{K nearest neighbours}
Calculating the Frechet distance is computationally expensive, given a path of length $m$
and a path of length $n$, it takes $O(mn)$ time to compute the distance. As such, computing the Frechet distance for between every test path against every training path is computationally infeasible. Instead we would like to quickly eliminate the majority of training paths without taking their Frechet distance from the test path. Solutions to the k nearest neighbours problem such as the ball tree naturally lend themselves to this. Unfortunately we can't build a ball tree using our distance function as a metric since we truncate paths before applying the Frechet distance, our distance function does not induce a metric space \footnote{Specifically, the distance function does not satisfy the identity of indiscernible and triangle inequality.} making it unsuitable for KNN.

Instead we use the distance from the starting point of two paths as our metric in the ball tree. We then grab the 10,000 paths with starting point closest to our test path and get the path with the closest Frechet distance of the test path out of all those paths. In practice this works very well since it gives the starting point of a path a higher precedence than other points.
\subsection{Memory and time constraints}
To reduce the running time of our algorithm we wished to keep disk I/O to a minimum, hence we loaded every single data-point into memory before operating on it. Unfortunately this meant that python's native datatypes could no longer be used as our program would require greater than 12Gb to run. We thus had to make use of efficient arrays and take care not to accidentally expand too much data into python's native floats at any one point.

\section{Results}
Results using Naive Bayes were not promising and as such weren't extensively tested. 

We ran a 10 fold cross validation test on a regression tree with maximum depth of: 5, 20, and 40 and on AdaBoost with 5 instances of a regression tree of max depth of 20. The results were as follows:

\begin{table}
\centering
\begin{tabular}{ |c|c|c|c|c|c| }
  \hline
  Algorithm & Tree 5 & Tree 20 & Tree 40 & AdaBoost & Baseline\\ 
  \hline                      
  Error & 2 & 3 & 3&2&s \\
  \hline  
\end{tabular}
\end{table}

\section{Further Work}

\section{Conclusion}

\end{document}
